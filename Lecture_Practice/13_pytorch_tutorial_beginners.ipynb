{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Tutorial for Beginners\n",
    "\n",
    "Welcome to this comprehensive PyTorch tutorial! This notebook is designed for students who are new to PyTorch and deep learning.\n",
    "\n",
    "## What is PyTorch?\n",
    "\n",
    "PyTorch is an open-source machine learning framework developed by Meta (formerly Facebook). It's widely used for:\n",
    "- Deep learning research and development\n",
    "- Computer vision tasks\n",
    "- Natural language processing\n",
    "- Scientific computing with GPU acceleration\n",
    "\n",
    "## Why PyTorch?\n",
    "\n",
    "- **Dynamic computation graphs**: Build models on-the-fly\n",
    "- **Pythonic**: Feels natural to Python developers\n",
    "- **Strong community**: Excellent documentation and support\n",
    "- **Research-friendly**: Easy to experiment and prototype\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation Check\n",
    "\n",
    "First, let's make sure PyTorch is installed and check our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PyTorch if not already installed\n",
    "# Uncomment the line below if you need to install PyTorch\n",
    "# !pip install torch torchvision torchaudio\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Python version: {torch.version.cuda if torch.cuda.is_available() else 'CUDA not available'}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding Tensors\n",
    "\n",
    "Tensors are the fundamental data structure in PyTorch. They're similar to NumPy arrays but can run on GPUs and support automatic differentiation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Creating Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors from Python lists\n",
    "data = [[1, 2], [3, 4]]\n",
    "tensor_from_list = torch.tensor(data)\n",
    "print(\"From list:\")\n",
    "print(tensor_from_list)\n",
    "print(f\"Shape: {tensor_from_list.shape}\")\n",
    "print(f\"Data type: {tensor_from_list.dtype}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors with specific shapes\n",
    "zeros = torch.zeros(2, 3)  # 2x3 matrix of zeros\n",
    "ones = torch.ones(2, 3)    # 2x3 matrix of ones\n",
    "random = torch.randn(2, 3) # 2x3 matrix with random values from standard normal distribution\n",
    "\n",
    "print(\"Zeros tensor:\")\n",
    "print(zeros)\n",
    "print(\"\\nOnes tensor:\")\n",
    "print(ones)\n",
    "print(\"\\nRandom tensor:\")\n",
    "print(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating tensors from NumPy arrays\n",
    "numpy_array = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "tensor_from_numpy = torch.from_numpy(numpy_array)\n",
    "\n",
    "print(\"NumPy array:\")\n",
    "print(numpy_array)\n",
    "print(\"\\nTensor from NumPy:\")\n",
    "print(tensor_from_numpy)\n",
    "print(f\"Data type: {tensor_from_numpy.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tensor Properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore tensor properties\n",
    "tensor = torch.randn(3, 4, 5)\n",
    "\n",
    "print(f\"Tensor shape: {tensor.shape}\")\n",
    "print(f\"Tensor size: {tensor.size()}\")\n",
    "print(f\"Number of dimensions: {tensor.ndim}\")\n",
    "print(f\"Data type: {tensor.dtype}\")\n",
    "print(f\"Device: {tensor.device}\")\n",
    "print(f\"Total number of elements: {tensor.numel()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Indexing and Slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample tensor\n",
    "tensor = torch.randn(4, 3)\n",
    "print(\"Original tensor:\")\n",
    "print(tensor)\n",
    "print()\n",
    "\n",
    "# Indexing (similar to NumPy)\n",
    "print(\"First row:\", tensor[0])\n",
    "print(\"First column:\", tensor[:, 0])\n",
    "print(\"Element at (1,2):\", tensor[1, 2])\n",
    "print(\"Last row:\", tensor[-1])\n",
    "print(\"First two rows:\", tensor[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Reshaping Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor and reshape it\n",
    "tensor = torch.arange(12)  # Creates tensor [0, 1, 2, ..., 11]\n",
    "print(\"Original tensor:\")\n",
    "print(tensor)\n",
    "print(f\"Shape: {tensor.shape}\")\n",
    "print()\n",
    "\n",
    "# Reshape to different dimensions\n",
    "reshaped = tensor.reshape(3, 4)\n",
    "print(\"Reshaped to 3x4:\")\n",
    "print(reshaped)\n",
    "print()\n",
    "\n",
    "# Reshape to 3D\n",
    "reshaped_3d = tensor.reshape(2, 3, 2)\n",
    "print(\"Reshaped to 2x3x2:\")\n",
    "print(reshaped_3d)\n",
    "print()\n",
    "\n",
    "# Using -1 to infer dimension\n",
    "auto_reshape = tensor.reshape(-1, 2)  # Let PyTorch figure out the first dimension\n",
    "print(\"Auto-reshaped to ?x2:\")\n",
    "print(auto_reshape)\n",
    "print(f\"Shape: {auto_reshape.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tensor Operations\n",
    "\n",
    "PyTorch provides many operations for manipulating tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Basic Mathematical Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample tensors\n",
    "a = torch.tensor([[1, 2], [3, 4]], dtype=torch.float32)\n",
    "b = torch.tensor([[5, 6], [7, 8]], dtype=torch.float32)\n",
    "\n",
    "print(\"Tensor a:\")\n",
    "print(a)\n",
    "print(\"\\nTensor b:\")\n",
    "print(b)\n",
    "print()\n",
    "\n",
    "# Element-wise operations\n",
    "print(\"Addition (a + b):\")\n",
    "print(a + b)\n",
    "print()\n",
    "\n",
    "print(\"Subtraction (a - b):\")\n",
    "print(a - b)\n",
    "print()\n",
    "\n",
    "print(\"Element-wise multiplication (a * b):\")\n",
    "print(a * b)\n",
    "print()\n",
    "\n",
    "print(\"Element-wise division (a / b):\")\n",
    "print(a / b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Matrix Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "print(\"Matrix multiplication (a @ b):\")\n",
    "print(a @ b)\n",
    "print()\n",
    "\n",
    "# Alternative syntax for matrix multiplication\n",
    "print(\"Matrix multiplication (torch.mm(a, b)):\")\n",
    "print(torch.mm(a, b))\n",
    "print()\n",
    "\n",
    "# Transpose\n",
    "print(\"Transpose of a:\")\n",
    "print(a.T)\n",
    "print()\n",
    "\n",
    "# Other useful operations\n",
    "print(\"Sum of all elements in a:\")\n",
    "print(a.sum())\n",
    "print()\n",
    "\n",
    "print(\"Sum along rows (axis=0):\")\n",
    "print(a.sum(dim=0))\n",
    "print()\n",
    "\n",
    "print(\"Sum along columns (axis=1):\")\n",
    "print(a.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 In-place vs Out-of-place Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Out-of-place operations (create new tensor)\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Original x:\", x)\n",
    "print(\"x.add(5):\", x.add(5))  # Creates new tensor\n",
    "print(\"x after add:\", x)  # x is unchanged\n",
    "print()\n",
    "\n",
    "# In-place operations (modify existing tensor)\n",
    "y = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Original y:\", y)\n",
    "y.add_(5)  # In-place operation (note the underscore)\n",
    "print(\"y after add_:\", y)  # y is modified\n",
    "print()\n",
    "\n",
    "# Warning: Be careful with in-place operations when computing gradients!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Automatic Differentiation (Autograd)\n",
    "\n",
    "PyTorch's autograd system automatically computes gradients, which is essential for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tensor that requires gradients\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "print(f\"x = {x}\")\n",
    "print(f\"x.requires_grad = {x.requires_grad}\")\n",
    "print()\n",
    "\n",
    "# Define a function y = x^2 + 3x + 1\n",
    "y = x**2 + 3*x + 1\n",
    "print(f\"y = x^2 + 3x + 1 = {y}\")\n",
    "print()\n",
    "\n",
    "# Compute gradients\n",
    "y.backward()  # This computes dy/dx\n",
    "print(f\"dy/dx = {x.grad}\")\n",
    "\n",
    "# Mathematical verification: dy/dx = 2x + 3 = 2(2) + 3 = 7\n",
    "print(f\"Expected gradient: 2*{x.item()} + 3 = {2*x.item() + 3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More complex example with multiple variables\n",
    "x = torch.tensor([1.0, 2.0], requires_grad=True)\n",
    "y = torch.tensor([3.0, 4.0], requires_grad=True)\n",
    "\n",
    "# Define z = x * y + x^2\n",
    "z = x * y + x**2\n",
    "print(f\"x = {x}\")\n",
    "print(f\"y = {y}\")\n",
    "print(f\"z = x * y + x^2 = {z}\")\n",
    "print()\n",
    "\n",
    "# We need to sum z to get a scalar for backward()\n",
    "loss = z.sum()\n",
    "print(f\"loss = sum(z) = {loss}\")\n",
    "print()\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "print(f\"dL/dx = {x.grad}\")\n",
    "print(f\"dL/dy = {y.grad}\")\n",
    "\n",
    "# Mathematical verification:\n",
    "# dz/dx = y + 2x, so dL/dx = [3+2*1, 4+2*2] = [5, 8]\n",
    "# dz/dy = x, so dL/dy = [1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Gradient Accumulation and Zeroing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradients accumulate by default\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "# First computation\n",
    "y1 = x**2\n",
    "y1.backward()\n",
    "print(f\"After first backward: x.grad = {x.grad}\")\n",
    "\n",
    "# Second computation (gradients accumulate!)\n",
    "y2 = x**3\n",
    "y2.backward()\n",
    "print(f\"After second backward: x.grad = {x.grad}\")\n",
    "print(\"Notice how gradients accumulated!\")\n",
    "print()\n",
    "\n",
    "# Zero gradients before next computation\n",
    "x.grad.zero_()\n",
    "print(f\"After zeroing: x.grad = {x.grad}\")\n",
    "\n",
    "# Third computation\n",
    "y3 = x**2\n",
    "y3.backward()\n",
    "print(f\"After third backward: x.grad = {x.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Building Neural Networks with torch.nn\n",
    "\n",
    "The `torch.nn` module provides building blocks for creating neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Basic Building Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear layer (fully connected layer)\n",
    "linear = nn.Linear(in_features=3, out_features=2)\n",
    "print(\"Linear layer:\")\n",
    "print(linear)\n",
    "print(f\"Weight shape: {linear.weight.shape}\")\n",
    "print(f\"Bias shape: {linear.bias.shape}\")\n",
    "print()\n",
    "\n",
    "# Test the linear layer\n",
    "x = torch.randn(1, 3)  # Batch size 1, 3 features\n",
    "output = linear(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "x = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# ReLU activation\n",
    "relu = nn.ReLU()\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"ReLU: {relu(x)}\")\n",
    "print()\n",
    "\n",
    "# Sigmoid activation\n",
    "sigmoid = nn.Sigmoid()\n",
    "print(f\"Sigmoid: {sigmoid(x)}\")\n",
    "print()\n",
    "\n",
    "# Tanh activation\n",
    "tanh = nn.Tanh()\n",
    "print(f\"Tanh: {tanh(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Creating a Simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # Define layers\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Define forward pass\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        return out\n",
    "\n",
    "# Create the network\n",
    "model = SimpleNet(input_size=4, hidden_size=10, output_size=3)\n",
    "print(\"Model architecture:\")\n",
    "print(model)\n",
    "print()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "print(f\"Trainable parameters: {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "x = torch.randn(5, 4)  # Batch of 5 samples, each with 4 features\n",
    "output = model(x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(\"\\nFirst few outputs:\")\n",
    "print(output[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Alternative Way to Build Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using nn.Sequential for simpler models\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(4, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 3)\n",
    ")\n",
    "\n",
    "print(\"Sequential model:\")\n",
    "print(model_sequential)\n",
    "print()\n",
    "\n",
    "# Test the sequential model\n",
    "output_seq = model_sequential(x)\n",
    "print(f\"Sequential output shape: {output_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training a Neural Network\n",
    "\n",
    "Now let's learn how to train a neural network with a complete example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss functions\n",
    "# For regression\n",
    "mse_loss = nn.MSELoss()\n",
    "mae_loss = nn.L1Loss()\n",
    "\n",
    "# For classification\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "binary_cross_entropy = nn.BCELoss()\n",
    "\n",
    "print(\"Loss functions created successfully!\")\n",
    "print()\n",
    "\n",
    "# Optimizers\n",
    "model = SimpleNet(4, 10, 3)\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01)\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Optimizers:\")\n",
    "print(f\"SGD: {optimizer_sgd}\")\n",
    "print(f\"Adam: {optimizer_adam}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Training Loop Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some dummy data for demonstration\n",
    "def create_dummy_data(num_samples=1000):\n",
    "    X = torch.randn(num_samples, 4)\n",
    "    # Create labels based on a simple rule\n",
    "    y = (X[:, 0] + X[:, 1] > 0).long()  # Binary classification\n",
    "    return X, y\n",
    "\n",
    "# Create data\n",
    "X_train, y_train = create_dummy_data(800)\n",
    "X_val, y_val = create_dummy_data(200)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {y_train.shape}\")\n",
    "print(f\"Validation data shape: {X_val.shape}\")\n",
    "print(f\"Validation labels shape: {y_val.shape}\")\n",
    "print(f\"Unique labels: {torch.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model for binary classification\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(4, 10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10, 2)  # 2 classes for binary classification\n",
    ")\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()  # Set model to training mode\n",
    "    \n",
    "    # Forward pass\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()  # Clear gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update parameters\n",
    "    \n",
    "    # Validation phase\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient computation for efficiency\n",
    "        val_outputs = model(X_val)\n",
    "        val_loss = criterion(val_outputs, y_val)\n",
    "    \n",
    "    # Store losses\n",
    "    train_losses.append(loss.item())\n",
    "    val_losses.append(val_loss.item())\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}')\n",
    "\n",
    "print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Training accuracy\n",
    "    train_outputs = model(X_train)\n",
    "    train_predictions = torch.argmax(train_outputs, dim=1)\n",
    "    train_accuracy = (train_predictions == y_train).float().mean()\n",
    "    \n",
    "    # Validation accuracy\n",
    "    val_outputs = model(X_val)\n",
    "    val_predictions = torch.argmax(val_outputs, dim=1)\n",
    "    val_accuracy = (val_predictions == y_val).float().mean()\n",
    "\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f} ({val_accuracy*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Complete Example: Iris Dataset Classification\n",
    "\n",
    "Let's put everything together with a real dataset - the famous Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "print(f\"Feature names: {iris.feature_names}\")\n",
    "print(f\"Class names: {iris.target_names}\")\n",
    "print()\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_train_tensor = torch.LongTensor(y_train)\n",
    "y_test_tensor = torch.LongTensor(y_test)\n",
    "\n",
    "print(f\"Training set size: {X_train_tensor.shape[0]}\")\n",
    "print(f\"Test set size: {X_test_tensor.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network for Iris classification\n",
    "class IrisNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IrisNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 16)  # 4 input features\n",
    "        self.fc2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(8, 3)   # 3 output classes\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create the model\n",
    "iris_model = IrisNet()\n",
    "print(\"Iris Classification Model:\")\n",
    "print(iris_model)\n",
    "print()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(iris_model.parameters(), lr=0.01)\n",
    "\n",
    "print(f\"Total parameters: {sum(p.numel() for p in iris_model.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Iris model\n",
    "num_epochs = 200\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "iris_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = iris_model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    with torch.no_grad():\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        accuracy = (predicted == y_train_tensor).float().mean()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "    train_accuracies.append(accuracy.item())\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}, Accuracy: {accuracy.item():.4f}')\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot loss\n",
    "ax1.plot(train_losses)\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot accuracy\n",
    "ax2.plot(train_accuracies)\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "iris_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = iris_model(X_test_tensor)\n",
    "    test_predictions = torch.argmax(test_outputs, dim=1)\n",
    "    test_accuracy = (test_predictions == y_test_tensor).float().mean()\n",
    "    \n",
    "    # Get probabilities using softmax\n",
    "    test_probabilities = torch.softmax(test_outputs, dim=1)\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print()\n",
    "\n",
    "# Show some predictions\n",
    "print(\"Sample predictions:\")\n",
    "print(\"Actual -> Predicted (Probabilities)\")\n",
    "for i in range(min(10, len(y_test))):\n",
    "    actual = iris.target_names[y_test[i]]\n",
    "    predicted = iris.target_names[test_predictions[i]]\n",
    "    probs = test_probabilities[i].numpy()\n",
    "    print(f\"{actual:10} -> {predicted:10} {probs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on new data\n",
    "def predict_iris_class(model, scaler, features):\n",
    "    \"\"\"\n",
    "    Predict iris class for given features\n",
    "    features: [sepal_length, sepal_width, petal_length, petal_width]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Normalize features\n",
    "        features_scaled = scaler.transform([features])\n",
    "        features_tensor = torch.FloatTensor(features_scaled)\n",
    "        \n",
    "        # Make prediction\n",
    "        output = model(features_tensor)\n",
    "        probabilities = torch.softmax(output, dim=1)\n",
    "        predicted_class = torch.argmax(output, dim=1)\n",
    "        \n",
    "        return predicted_class.item(), probabilities.numpy()[0]\n",
    "\n",
    "# Example predictions\n",
    "examples = [\n",
    "    [5.1, 3.5, 1.4, 0.2],  # Typical setosa\n",
    "    [6.2, 2.8, 4.8, 1.8],  # Typical versicolor\n",
    "    [7.2, 3.0, 5.8, 1.6],  # Typical virginica\n",
    "]\n",
    "\n",
    "print(\"Predictions for new samples:\")\n",
    "print(\"Features [SL, SW, PL, PW] -> Prediction (Probabilities)\")\n",
    "for features in examples:\n",
    "    pred_class, probs = predict_iris_class(iris_model, scaler, features)\n",
    "    class_name = iris.target_names[pred_class]\n",
    "    print(f\"{features} -> {class_name} {probs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Best Practices and Next Steps\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Tensors are fundamental** - They're like NumPy arrays but with GPU support and automatic differentiation\n",
    "\n",
    "2. **Autograd is powerful** - PyTorch automatically computes gradients for you\n",
    "\n",
    "3. **nn.Module is the base** - Always inherit from this when building models\n",
    "\n",
    "4. **Training loop structure**:\n",
    "   - Forward pass\n",
    "   - Compute loss\n",
    "   - Zero gradients\n",
    "   - Backward pass\n",
    "   - Update parameters\n",
    "\n",
    "5. **Don't forget to**:\n",
    "   - Set model to train/eval mode\n",
    "   - Use `torch.no_grad()` for inference\n",
    "   - Normalize your data\n",
    "   - Monitor training progress\n",
    "\n",
    "### Common Debugging Tips:\n",
    "\n",
    "- Check tensor shapes frequently\n",
    "- Ensure data types match (float32 for inputs, long for classification labels)\n",
    "- Watch out for in-place operations when computing gradients\n",
    "- Use `model.train()` and `model.eval()` appropriately\n",
    "- Don't forget to zero gradients before backward pass\n",
    "\n",
    "### Next Steps to Learn:\n",
    "\n",
    "1. **Data Loading**: `torch.utils.data.DataLoader` for efficient batch processing\n",
    "2. **Convolutional Networks**: For computer vision tasks\n",
    "3. **Recurrent Networks**: For sequence data\n",
    "4. **Transfer Learning**: Using pre-trained models\n",
    "5. **GPU Computing**: Moving tensors and models to GPU\n",
    "6. **Saving/Loading Models**: Model persistence\n",
    "\n",
    "### Useful Resources:\n",
    "\n",
    "- [PyTorch Documentation](https://pytorch.org/docs/stable/index.html)\n",
    "- [PyTorch Tutorials](https://pytorch.org/tutorials/)\n",
    "- [PyTorch Examples](https://github.com/pytorch/examples)\n",
    "- [Deep Learning with PyTorch](https://pytorch.org/deep-learning-with-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Exercise for You!\n",
    "\n",
    "Try modifying the Iris classification example:\n",
    "\n",
    "1. Change the network architecture (add more layers, change sizes)\n",
    "2. Try different activation functions (Tanh, LeakyReLU)\n",
    "3. Experiment with different optimizers and learning rates\n",
    "4. Add regularization techniques (different dropout rates)\n",
    "5. Create visualizations of the decision boundaries\n",
    "\n",
    "Happy learning with PyTorch! ðŸš€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}