{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) and Its Variants\n",
    "\n",
    "---\n",
    "\n",
    "## What is Gradient Descent?\n",
    "\n",
    "**Gradient Descent** is an optimization algorithm used to minimize a cost function by iteratively moving in the direction of steepest descent (negative gradient).\n",
    "\n",
    "### Key Concepts:\n",
    "- **Cost Function (J)**: Measures how wrong our predictions are\n",
    "- **Gradient (∇J)**: Direction of steepest increase in cost\n",
    "- **Learning Rate (α)**: Step size in each iteration\n",
    "- **Convergence**: When we reach the minimum (or close enough)\n",
    "\n",
    "### Types of Gradient Descent:\n",
    "\n",
    "1. **Batch Gradient Descent**: Uses entire dataset for each update\n",
    "2. **Stochastic Gradient Descent (SGD)**: Uses one sample at a time\n",
    "3. **Mini-batch Gradient Descent**: Uses small batches of samples\n",
    "\n",
    "---\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Linear Regression Cost Function (Mean Squared Error):\n",
    "```\n",
    "J(w, b) = (1/2m) × Σ(h(x_i) - y_i)²\n",
    "\n",
    "where:\n",
    "h(x_i) = w₁x₁ + w₂x₂ + ... + wₙxₙ + b  (prediction)\n",
    "m = number of training examples\n",
    "```\n",
    "\n",
    "### Gradient Computation:\n",
    "```\n",
    "∂J/∂w_j = (1/m) × Σ(h(x_i) - y_i) × x_ij\n",
    "∂J/∂b = (1/m) × Σ(h(x_i) - y_i)\n",
    "```\n",
    "\n",
    "### Parameter Updates:\n",
    "```\n",
    "w_j := w_j - α × ∂J/∂w_j\n",
    "b := b - α × ∂J/∂b\n",
    "```\n",
    "\n",
    "### SGD vs Batch GD:\n",
    "- **Batch GD**: Updates using all m examples\n",
    "- **SGD**: Updates using 1 example at a time\n",
    "- **Mini-batch**: Updates using k examples (1 < k < m)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_regression, load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete GradientDescentRegressor Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientDescentRegressor:\n",
    "    def __init__(self, learning_rate=0.01, max_iterations=1000, tolerance=1e-6):\n",
    "        \"\"\"\n",
    "        Initialize the Gradient Descent Regressor\n",
    "        \n",
    "        Parameters:\n",
    "        learning_rate: Step size for parameter updates\n",
    "        max_iterations: Maximum number of iterations\n",
    "        tolerance: Stopping criterion based on cost change\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.tolerance = tolerance\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "    \n",
    "    def _add_intercept(self, X):\n",
    "        \"\"\"Add bias column to feature matrix\"\"\"\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "    \n",
    "    def _compute_cost(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Compute Mean Squared Error cost\n",
    "        \n",
    "        Parameters:\n",
    "        X: Feature matrix with intercept (n_samples, n_features + 1)\n",
    "        y: Target values (n_samples,)\n",
    "        weights: Current weights including bias (n_features + 1,)\n",
    "        \n",
    "        Returns:\n",
    "        cost: Mean squared error\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # number of samples\n",
    "        predictions = X @ weights\n",
    "        # cost = ?\n",
    "        return cost\n",
    "    \n",
    "    def _compute_gradients(self, X, y, weights):\n",
    "        \"\"\"\n",
    "        Compute gradients for all parameters\n",
    "        \n",
    "        Parameters:\n",
    "        X: Feature matrix with intercept (n_samples, n_features + 1)\n",
    "        y: Target values (n_samples,)\n",
    "        weights: Current weights including bias (n_features + 1,)\n",
    "        \n",
    "        Returns:\n",
    "        gradients: Gradient vector (n_features + 1,)\n",
    "        \"\"\"\n",
    "        m = X.shape[0]  # number of samples\n",
    "        predictions = X @ weights\n",
    "        # errors = \n",
    "        # gradients for linear regression\n",
    "        return gradients\n",
    "    \n",
    "    def fit_batch_gd(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit using Batch Gradient Descent\n",
    "        \"\"\"\n",
    "        # Add intercept term\n",
    "        X_with_intercept = self._add_intercept(X)\n",
    "        n_features = X_with_intercept.shape[1]\n",
    "        \n",
    "        # Initialize weights\n",
    "        weights = np.random.randn(n_features) * 0.01\n",
    "        \n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Compute cost and gradients\n",
    "            # cost = \n",
    "            # gradients = \n",
    "            \n",
    "            # weights = \n",
    "            \n",
    "            # Store history\n",
    "            self.cost_history.append(cost)\n",
    "            self.weight_history.append(weights.copy())\n",
    "            \n",
    "            # Check for convergence\n",
    "            if len(self.cost_history) > 1:\n",
    "                if abs(self.cost_history[-2] - self.cost_history[-1]) < self.tolerance:\n",
    "                    print(f\"Batch GD converged after {iteration + 1} iterations\")\n",
    "                    break\n",
    "        \n",
    "        # Store final parameters\n",
    "        self.bias = weights[0]\n",
    "        self.weights = weights[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_sgd(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit using Stochastic Gradient Descent\n",
    "        \"\"\"\n",
    "        # Add intercept term\n",
    "        X_with_intercept =\n",
    "        n_samples, n_features = X_with_intercept.shape\n",
    "        \n",
    "        # Initialize weights\n",
    "        weights = np.random.randn(n_features) * 0.01\n",
    "        \n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            for i in indices:\n",
    "                # calculate the updated weights\n",
    "                xi = X_with_intercept[i:i+1]  # Single sample (keep 2D)\n",
    "                yi = y[i:i+1]\n",
    "                # weights = weights - self.learning_rate * gradient.flatten()\n",
    "            \n",
    "            # Compute cost after each epoch\n",
    "            # Update the cost and weight history\n",
    "            \n",
    "            # Check for convergence (less strict for SGD)\n",
    "            if len(self.cost_history) > 10:\n",
    "                recent_costs = self.cost_history[-10:]\n",
    "                if np.std(recent_costs) < self.tolerance:\n",
    "                    print(f\"SGD converged after {iteration + 1} iterations\")\n",
    "                    break\n",
    "        \n",
    "        # Store final parameters\n",
    "        self.bias = weights[0]\n",
    "        self.weights = weights[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_mini_batch_gd(self, X, y, batch_size=32):\n",
    "        \"\"\"\n",
    "        Fit using Mini-batch Gradient Descent\n",
    "        \"\"\"\n",
    "        # Add intercept term\n",
    "        X_with_intercept = self._add_intercept(X)\n",
    "        n_samples, n_features = X_with_intercept.shape\n",
    "        \n",
    "        # Initialize weights\n",
    "        weights = np.random.randn(n_features) * 0.01\n",
    "        \n",
    "        self.cost_history = []\n",
    "        self.weight_history = []\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Shuffle the data\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "\n",
    "                X_batch = X_with_intercept[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "                \n",
    "                weights = weights - self.learning_rate * gradients\n",
    "            \n",
    "            # Compute cost after each epoch and store cost and weight history\n",
    "            cost = self._compute_cost(X_with_intercept, y, weights)\n",
    "            \n",
    "            # Check for convergence\n",
    "            if len(self.cost_history) > 5:\n",
    "                recent_costs = self.cost_history[-5:]\n",
    "                if np.std(recent_costs) < self.tolerance:\n",
    "                    print(f\"Mini-batch GD converged after {iteration + 1} iterations\")\n",
    "                    break\n",
    "        \n",
    "        # Store final parameters\n",
    "        self.bias = weights[0]\n",
    "        self.weights = weights[1:]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def fit_adagrad(self, X, y, batch_size=32, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        AdaGrad - adapts learning rate based on historical gradients\n",
    "        \"\"\"\n",
    "        X_with_intercept = self._add_intercept(X)\n",
    "        n_samples, n_features = X_with_intercept.shape\n",
    "        \n",
    "        # Initialize weights and gradient accumulator\n",
    "        weights = np.random.randn(n_features) * 0.01\n",
    "        gradient_squared_sum = np.zeros_like(weights)  # Accumulates squared gradients\n",
    "        \n",
    "        self.cost_history = []\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                X_batch = X_with_intercept[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "\n",
    "                # Implement Adaptive learning rate and gradients\n",
    "                weights = weights - adaptive_lr * gradients\n",
    "            \n",
    "            cost = self._compute_cost(X_with_intercept, y, weights)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            if len(self.cost_history) > 5 and np.std(self.cost_history[-5:]) < 1e-6:\n",
    "                break\n",
    "        \n",
    "        self.bias = weights[0]\n",
    "        self.weights = weights[1:]\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on new data\n",
    "        \"\"\"\n",
    "        return X @ self.weights + self.bias\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Calculate R² score\n",
    "        \"\"\"\n",
    "        predictions = self.predict(X)\n",
    "        ss_res = np.sum((y - predictions) ** 2)\n",
    "        ss_tot = np.sum((y - np.mean(y)) ** 2)\n",
    "        return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Optimizers Class\n",
    "\n",
    "No need to solve, just read through"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedOptimizers(GradientDescentRegressor):\n",
    "    \"\"\"\n",
    "    Advanced optimization algorithms that inherit from GradientDescentRegressor\n",
    "    \"\"\"\n",
    "    \n",
    "    def fit_momentum(self, X, y, batch_size=32, momentum=0.9):\n",
    "        \"\"\"\n",
    "        Momentum SGD - accelerates SGD by adding a fraction of previous update\n",
    "        \"\"\"\n",
    "        X_with_intercept = self._add_intercept(X)\n",
    "        n_samples, n_features = X_with_intercept.shape\n",
    "        \n",
    "        # Initialize weights and velocity\n",
    "        weights = np.random.randn(n_features) * 0.01\n",
    "        velocity = np.zeros_like(weights)\n",
    "        \n",
    "        self.cost_history = []\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                X_batch = X_with_intercept[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "                \n",
    "                gradients = self._compute_gradients(X_batch, y_batch, weights)\n",
    "                velocity = momentum * velocity - self.learning_rate * gradients\n",
    "                weights = weights + velocity\n",
    "            \n",
    "            cost = self._compute_cost(X_with_intercept, y, weights)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            if len(self.cost_history) > 5 and np.std(self.cost_history[-5:]) < 1e-6:\n",
    "                break\n",
    "        \n",
    "        self.bias = weights[0]\n",
    "        self.weights = weights[1:]\n",
    "        return self\n",
    "    \n",
    "    def fit_adam(self, X, y, batch_size=32, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Adam - combines momentum with adaptive learning rates\n",
    "        \"\"\"\n",
    "        X_with_intercept = self._add_intercept(X)\n",
    "        n_samples, n_features = X_with_intercept.shape\n",
    "        \n",
    "        # Initialize weights and moments\n",
    "        weights = np.random.randn(n_features) * 0.01\n",
    "        m = np.zeros_like(weights)  # First moment (momentum)\n",
    "        v = np.zeros_like(weights)  # Second moment (RMSprop)\n",
    "        \n",
    "        self.cost_history = []\n",
    "        t = 0  # Time step\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            \n",
    "            for start_idx in range(0, n_samples, batch_size):\n",
    "                end_idx = min(start_idx + batch_size, n_samples)\n",
    "                batch_indices = indices[start_idx:end_idx]\n",
    "                \n",
    "                X_batch = X_with_intercept[batch_indices]\n",
    "                y_batch = y[batch_indices]\n",
    "                \n",
    "                t += 1\n",
    "                gradients = self._compute_gradients(X_batch, y_batch, weights)\n",
    "                \n",
    "                # Update biased first moment estimate\n",
    "                m = beta1 * m + (1 - beta1) * gradients\n",
    "                # Update biased second raw moment estimate\n",
    "                v = beta2 * v + (1 - beta2) * (gradients ** 2)\n",
    "                \n",
    "                # Compute bias-corrected first moment estimate\n",
    "                m_hat = m / (1 - beta1 ** t)\n",
    "                # Compute bias-corrected second raw moment estimate\n",
    "                v_hat = v / (1 - beta2 ** t)\n",
    "                \n",
    "                # Update weights\n",
    "                weights = weights - self.learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "            \n",
    "            cost = self._compute_cost(X_with_intercept, y, weights)\n",
    "            self.cost_history.append(cost)\n",
    "            \n",
    "            if len(self.cost_history) > 5 and np.std(self.cost_history[-5:]) < 1e-6:\n",
    "                break\n",
    "        \n",
    "        self.bias = weights[0]\n",
    "        self.weights = weights[1:]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Complete Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data\n",
    "X, y = make_regression(n_samples=200, n_features=2, noise=10, random_state=42)\n",
    "\n",
    "# Standardize features (important for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all implemented methods\n",
    "methods = {\n",
    "    'Batch GD': ('basic', 'fit_batch_gd'),\n",
    "    'Stochastic GD': ('basic', 'fit_sgd'), \n",
    "    'Mini-batch GD': ('basic', 'fit_mini_batch_gd'),\n",
    "    'AdaGrad': ('basic', 'fit_adagrad'),\n",
    "    'Momentum': ('advanced', 'fit_momentum'),\n",
    "    'Adam': ('advanced', 'fit_adam')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "learning_rate = 0.01\n",
    "max_iter = 100\n",
    "\n",
    "for name, (optimizer_type, method) in methods.items():\n",
    "    print(f\"\\nTraining with {name}...\")\n",
    "    \n",
    "    if optimizer_type == 'basic':\n",
    "        model = GradientDescentRegressor(learning_rate=learning_rate, max_iterations=max_iter)\n",
    "    else:\n",
    "        model = AdvancedOptimizers(learning_rate=learning_rate, max_iterations=max_iter)\n",
    "    \n",
    "    if method == 'fit_mini_batch_gd':\n",
    "        getattr(model, method)(X_train, y_train, batch_size=16)\n",
    "    elif method in ['fit_adagrad', 'fit_momentum', 'fit_adam']:\n",
    "        getattr(model, method)(X_train, y_train, batch_size=16)\n",
    "    else:\n",
    "        getattr(model, method)(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': model,\n",
    "        'train_score': train_score,\n",
    "        'test_score': test_score,\n",
    "        'final_cost': model.cost_history[-1] if model.cost_history else float('inf')\n",
    "    }\n",
    "    \n",
    "    print(f\"Train R²: {train_score:.4f}\")\n",
    "    print(f\"Test R²: {test_score:.4f}\")\n",
    "    print(f\"Final cost: {results[name]['final_cost']:.4f}\")\n",
    "    print(f\"Iterations: {len(model.cost_history)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize convergence comparison\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Cost history comparison\n",
    "plt.subplot(2, 3, 1)\n",
    "colors = ['blue', 'red', 'green', 'purple', 'orange', 'brown']\n",
    "for i, (name, result) in enumerate(results.items()):\n",
    "    if result['model'].cost_history:\n",
    "        plt.plot(result['model'].cost_history, color=colors[i], label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.title('Cost Function Convergence - All Methods')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Performance comparison\n",
    "plt.subplot(2, 3, 2)\n",
    "methods_list = list(results.keys())\n",
    "train_scores = [results[m]['train_score'] for m in methods_list]\n",
    "test_scores = [results[m]['test_score'] for m in methods_list]\n",
    "\n",
    "x = np.arange(len(methods_list))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, train_scores, width, label='Train R²', alpha=0.8)\n",
    "plt.bar(x + width/2, test_scores, width, label='Test R²', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('R² Score')\n",
    "plt.title('Performance Comparison')\n",
    "plt.xticks(x, methods_list, rotation=45, ha='right')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Final cost comparison\n",
    "plt.subplot(2, 3, 3)\n",
    "final_costs = [results[m]['final_cost'] for m in methods_list]\n",
    "bars = plt.bar(methods_list, final_costs, color=colors[:len(methods_list)], alpha=0.7)\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Final Cost')\n",
    "plt.title('Final Cost Comparison')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Convergence speed\n",
    "plt.subplot(2, 3, 4)\n",
    "iterations = [len(results[m]['model'].cost_history) for m in methods_list]\n",
    "bars = plt.bar(methods_list, iterations, color=colors[:len(methods_list)], alpha=0.7)\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Iterations to Convergence')\n",
    "plt.title('Convergence Speed')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Test performance ranking\n",
    "plt.subplot(2, 3, 5)\n",
    "sorted_methods = sorted(results.items(), key=lambda x: x[1]['test_score'], reverse=True)\n",
    "sorted_names = [name for name, _ in sorted_methods]\n",
    "sorted_scores = [result['test_score'] for _, result in sorted_methods]\n",
    "\n",
    "bars = plt.bar(range(len(sorted_names)), sorted_scores, \n",
    "              color=[colors[methods_list.index(name)] for name in sorted_names], alpha=0.7)\n",
    "plt.xlabel('Method (Ranked)')\n",
    "plt.ylabel('Test R² Score')\n",
    "plt.title('Performance Ranking')\n",
    "plt.xticks(range(len(sorted_names)), sorted_names, rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, sorted_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SOLUTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"All TODO items have been successfully implemented:\")\n",
    "print(\"✅ TODO 1: MSE cost function\")\n",
    "print(\"✅ TODO 2: Gradient computation\")\n",
    "print(\"✅ TODO 3: Batch GD weight updates\")\n",
    "print(\"✅ TODO 4: SGD single sample updates\")\n",
    "print(\"✅ TODO 5: Mini-batch GD updates\")\n",
    "print(\"✅ TODO 7: AdaGrad optimizer\")\n",
    "print(\"✅ BONUS: Added complete AdvancedOptimizers class with Momentum and Adam\")\n",
    "print(\"\\nBest performing method:\", sorted_names[0], f\"(R² = {sorted_scores[0]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Real Dataset: Diabetes Dataset\n",
    "\n",
    "Let's test our optimizers on a real regression dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load diabetes dataset\n",
    "diabetes = load_diabetes()\n",
    "X_diabetes, y_diabetes = diabetes.data, diabetes.target\n",
    "\n",
    "# Standardize features\n",
    "scaler_diabetes = StandardScaler()\n",
    "X_diabetes_scaled = scaler_diabetes.fit_transform(X_diabetes)\n",
    "\n",
    "# Split the data\n",
    "X_train_diab, X_test_diab, y_train_diab, y_test_diab = train_test_split(\n",
    "    X_diabetes_scaled, y_diabetes, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Diabetes dataset shape: {X_diabetes.shape}\")\n",
    "print(f\"Features: {len(diabetes.feature_names)}\")\n",
    "print(f\"Target range: [{y_diabetes.min():.2f}, {y_diabetes.max():.2f}]\")\n",
    "print(f\"Feature names: {diabetes.feature_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Adam optimizer on diabetes dataset\n",
    "print(\"Testing optimizers on Diabetes dataset...\\n\")\n",
    "\n",
    "diabetes_results = {}\n",
    "\n",
    "# Complete the Code\n",
    "for name, (optimizer_type, method) in key_optimizers.items():\n",
    "    if optimizer_type == 'basic':\n",
    "        model = GradientDescentRegressor(learning_rate=0.01, max_iterations=200)\n",
    "        getattr(model, method)(X_train_diab, y_train_diab)\n",
    "    else:\n",
    "        model = AdvancedOptimizers(learning_rate=0.01, max_iterations=200)\n",
    "        getattr(model, method)(X_train_diab, y_train_diab)\n",
    "    \n",
    "    train_score = model.score(X_train_diab, y_train_diab)\n",
    "    test_score = model.score(X_test_diab, y_test_diab)\n",
    "    \n",
    "    diabetes_results[name] = {\n",
    "        'model': model,\n",
    "        'train_score': train_score,\n",
    "        'test_score': test_score\n",
    "    }\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Train R²: {train_score:.4f}\")\n",
    "    print(f\"  Test R²: {test_score:.4f}\")\n",
    "    print(f\"  Iterations: {len(model.cost_history)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results on diabetes dataset\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Convergence curves\n",
    "plt.subplot(1, 3, 1)\n",
    "colors = ['blue', 'red', 'green']\n",
    "for i, (name, result) in enumerate(diabetes_results.items()):\n",
    "    plt.plot(result['model'].cost_history, color=colors[i], label=name, linewidth=2)\n",
    "\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Cost (MSE)')\n",
    "plt.title('Diabetes Dataset - Convergence')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.yscale('log')\n",
    "\n",
    "# Plot 2: Performance comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "methods = list(diabetes_results.keys())\n",
    "test_scores = [diabetes_results[m]['test_score'] for m in methods]\n",
    "\n",
    "bars = plt.bar(methods, test_scores, color=['blue', 'red', 'green'], alpha=0.7)\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Test R² Score')\n",
    "plt.title('Test Performance on Diabetes')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "for bar, score in zip(bars, test_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "             f'{score:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# Plot 3: Predictions vs Actual (for best model)\n",
    "plt.subplot(1, 3, 3)\n",
    "best_model_name = max(diabetes_results.keys(), \n",
    "                     key=lambda x: diabetes_results[x]['test_score'])\n",
    "best_model = diabetes_results[best_model_name]['model']\n",
    "\n",
    "y_pred = best_model.predict(X_test_diab)\n",
    "plt.scatter(y_test_diab, y_pred, alpha=0.6, color='blue')\n",
    "plt.plot([y_test_diab.min(), y_test_diab.max()], \n",
    "         [y_test_diab.min(), y_test_diab.max()], 'r--', linewidth=2)\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Predicted Values')\n",
    "plt.title(f'Predictions vs Actual ({best_model_name})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Best performing model: {best_model_name} (R² = {diabetes_results[best_model_name]['test_score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learning Rate Experiments\n",
    "\n",
    "Explore how different learning rates affect convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment with different learning rates\n",
    "learning_rates = [0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "lr_results = {}\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, lr in enumerate(learning_rates, 1):\n",
    "    try:\n",
    "        # Use SGD for this experiment\n",
    "        model = GradientDescentRegressor(learning_rate=lr, max_iterations=100)\n",
    "        model.fit_sgd(X_train, y_train)\n",
    "        \n",
    "        test_score = model.score(X_test, y_test)\n",
    "        lr_results[lr] = {\n",
    "            'model': model,\n",
    "            'test_score': test_score,\n",
    "            'converged': len(model.cost_history) < 100\n",
    "        }\n",
    "        \n",
    "        # Plot learning curve\n",
    "        plt.subplot(2, 3, i)\n",
    "        plt.plot(model.cost_history, linewidth=2, color='blue')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Cost')\n",
    "        plt.title(f'LR = {lr}\\nR² = {test_score:.3f}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        if not lr_results[lr]['converged']:\n",
    "            plt.title(f'LR = {lr} (No Convergence)\\nR² = {test_score:.3f}', color='red')\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Learning rate {lr} failed: {str(e)}\")\n",
    "        lr_results[lr] = {'test_score': 0, 'converged': False}\n",
    "\n",
    "# Summary plot\n",
    "plt.subplot(2, 3, 6)\n",
    "lrs = list(lr_results.keys())\n",
    "scores = [lr_results[lr]['test_score'] for lr in lrs]\n",
    "colors = ['red' if not lr_results[lr]['converged'] else 'blue' for lr in lrs]\n",
    "\n",
    "bars = plt.bar(range(len(lrs)), scores, color=colors, alpha=0.7)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Test R² Score')\n",
    "plt.title('Learning Rate vs Performance')\n",
    "plt.xticks(range(len(lrs)), [str(lr) for lr in lrs])\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nLearning Rate Analysis:\")\n",
    "print(\"-\" * 50)\n",
    "for lr, result in lr_results.items():\n",
    "    status = \"✓ Converged\" if result['converged'] else \"✗ No convergence\"\n",
    "    print(f\"LR = {lr:5.3f}: R² = {result['test_score']:6.3f} ({status})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "---\n",
    "\n",
    "## Discussion Questions\n",
    "\n",
    "After completing the exercises, discuss the following:\n",
    "\n",
    "### 1. **Gradient Descent Variants**\n",
    "- What are the trade-offs between Batch, Stochastic, and Mini-batch GD?\n",
    "- When would you choose each variant?\n",
    "- How does batch size affect convergence and computational efficiency?\n",
    "\n",
    "### 2. **Learning Rate Selection**\n",
    "- What happens with learning rates that are too high or too low?\n",
    "- How can you detect if your learning rate is inappropriate?\n",
    "- What strategies exist for adaptive learning rate scheduling?\n",
    "\n",
    "### 3. **Feature Scaling**\n",
    "- Why is feature standardization crucial for gradient descent?\n",
    "- What happens when features have very different scales?\n",
    "- How does this relate to the condition number of the optimization problem?\n",
    "\n",
    "### 4. **Practical Considerations**\n",
    "- How do you choose hyperparameters in practice?\n",
    "- What are the computational trade-offs of different optimizers?\n",
    "- How does this connect to deep learning and neural networks?\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **SGD** trades accuracy for speed and can handle large datasets\n",
    "- **Adaptive methods** like Adam adjust learning rates automatically\n",
    "- **Feature scaling** is critical for gradient descent performance\n",
    "- **Learning rate** is the most important hyperparameter to tune\n",
    "- **Mini-batch** often provides the best balance of speed and stability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
